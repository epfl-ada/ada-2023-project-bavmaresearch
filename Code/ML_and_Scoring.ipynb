{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "try:\n",
    "    # Reading the data from a CSV file into a pandas DataFrame\n",
    "    df_avec_successful = pd.read_csv('../Datasets/movies_cleaned_dataset.csv')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"File not found: {e}\")\n",
    "    # Handle the exception (e.g., exit the script or log an error)\n",
    "    exit()\n",
    "\n",
    "# Filling missing values for specific columns with their mean\n",
    "columns_to_fill = ['Inflation_adjusted_profit', 'averageRating', 'Oscar_Wins', 'Nominations']\n",
    "for column in columns_to_fill:\n",
    "    df_avec_successful[column].fillna(df_avec_successful[column].mean(), inplace=True)\n",
    "\n",
    "# Capping extreme values in 'Inflation adjusted profit' to reduce outlier impact\n",
    "# This reduces the impact of outliers by setting a threshold (90th percentile here)\n",
    "# Values above this threshold are set to the threshold value itself\n",
    "cap_threshold = df_avec_successful['Inflation_adjusted_profit'].quantile(0.90)\n",
    "df_avec_successful['capped_profit'] = df_avec_successful['Inflation_adjusted_profit'].clip(upper=cap_threshold)\n",
    "\n",
    "# Standardizing selected features (z-score normalization)\n",
    "z_features = {\n",
    "    'capped_profit': zscore(df_avec_successful['capped_profit']),\n",
    "    'averageRating': zscore(df_avec_successful['averageRating']),\n",
    "    'Oscar_Wins': zscore(df_avec_successful['Oscar_Wins']),\n",
    "    'Nominations': zscore(df_avec_successful['Nominations'])\n",
    "}\n",
    "\n",
    "# Assigning weights to standardized features\n",
    "weights = {\n",
    "    'capped_profit': 0.3,\n",
    "    'averageRating': 0.35,\n",
    "    'Oscar_Wins': 0.175,\n",
    "    'Nominations': 0.175\n",
    "}\n",
    "\n",
    "# Calculating a composite 'Successful' score using weighted features\n",
    "df_avec_successful['Successful'] = sum(weights[feature] * z_features[feature] for feature in weights)\n",
    "\n",
    "# Normalizing the 'Successful' score to a 0-10 scale for interpretability\n",
    "min_score = df_avec_successful['Successful'].min()\n",
    "max_score = df_avec_successful['Successful'].max()\n",
    "df_avec_successful['Successful'] = round((df_avec_successful['Successful'] - min_score) / (max_score - min_score) * 10, 1)\n",
    "\n",
    "# Sorting the DataFrame by 'Successful' score in descending order\n",
    "df_avec_successful.sort_values(by='Successful', ascending=False, inplace=True)\n",
    "\n",
    "# Display the top 300 rows\n",
    "df_avec_successful.head(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avec_successful.describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer_feature_names(column_transformer):\n",
    "    \"\"\"\n",
    "    Get feature names from a fitted ColumnTransformer.\n",
    "    \"\"\"\n",
    "    output_features = []\n",
    "\n",
    "    for name, pipe, features in column_transformer.transformers_:\n",
    "        if name == 'remainder':\n",
    "            # If the remainder is a passthrough, its feature names are the same as the column names\n",
    "            if pipe == 'passthrough':\n",
    "                output_features.extend(features)\n",
    "            continue\n",
    "\n",
    "        # For transformers with a get_feature_names_out method\n",
    "        if hasattr(pipe, 'get_feature_names_out'):\n",
    "            transformer_features = pipe.get_feature_names_out(features)\n",
    "        else:\n",
    "            transformer_features = features\n",
    "\n",
    "        output_features.extend(transformer_features)\n",
    "\n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import xgboost as xgb \n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "#from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "data = df_avec_successful\n",
    "X = data.drop('Successful', axis=1)\n",
    "y = data['Successful']  # target\n",
    "\n",
    "#preprocessing steps for both categorical and numeric data. Categorical features are filled with a\n",
    "#placeholder value for any missing data and then one-hot encoded. Numeric features are imputed with their\n",
    "#mean and then standardized. This transformed data is then used to train the RandomForestRegressor.\n",
    "#Make sure to adjust the categorical_columns and numeric_columns lists to include all relevant features from your dataset.\n",
    "#Selecting categorical and numeric columns\n",
    "\n",
    "categorical_columns = ['Main_language', 'Main_country']  \n",
    "numeric_columns = ['Movie_runtime']  \n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())  # standardizing data\n",
    "])\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_columns)\n",
    "    ])\n",
    "\n",
    "# Get a list of unique genres\n",
    "top_genres = data['Top_genres'].value_counts().index\n",
    "print(top_genres)\n",
    "print(data['Top_genres'].value_counts())\n",
    "\n",
    "#OPTIMISATION\n",
    "\n",
    "# Hyperparameters grid for Random Forest\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [10, 20, 30],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "# Hyperparameters grid for CatBoost\n",
    "param_grid_catboost = {\n",
    "    'classifier__iterations': [100, 500, 1000],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.3],\n",
    "    'classifier__depth': [4, 6, 10],\n",
    "}\n",
    "# Analysis for each genre\n",
    "for genre in top_genres:\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    print(f\"Analyzing genre: {genre} - Data Points: {len(data[data['Top_genres'] == genre])}\")\n",
    "\n",
    "    genre_data = data[data['Top_genres'] == genre]\n",
    "    genre_data.loc[genre_data['Main_language'] == 'Multilingual', 'Main_language'] = 'English Language'\n",
    "\n",
    "    # Filter movies with success score greater than 6\n",
    "    successful_movies = genre_data[genre_data['Successful'] >= 6]\n",
    "\n",
    "    # Create a copy of the DataFrame to avoid SettingWithCopyWarning\n",
    "    successful_movies = successful_movies.copy()\n",
    "\n",
    "    # Perform your operation\n",
    "    successful_movies['Movie_runtime'] = pd.to_numeric(successful_movies['Movie_runtime'], errors='coerce')\n",
    "\n",
    "    mean_runtime = successful_movies['Movie_runtime'].dropna().mean()\n",
    "    print(f\"Optimal Runtime: {mean_runtime:.2f} minutes\")\n",
    "\n",
    "    try:\n",
    "        X_genre = genre_data[categorical_columns]\n",
    "        y_genre = genre_data['Successful']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_genre, y_genre, test_size=0.1, random_state=42)\n",
    "\n",
    "        clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('classifier', CatBoostRegressor(random_state=42, verbose=0))])\n",
    "\n",
    "        # Grid search for hyperparameter tuning\n",
    "        grid_search = GridSearchCV(clf, param_grid_catboost, cv=3, scoring='neg_mean_squared_error')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Best model after grid search\n",
    "        best_clf = grid_search.best_estimator_\n",
    "\n",
    "        # Predict and evaluate\n",
    "        y_pred = best_clf.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = mse ** 0.5\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"MSE: {mse}, RMSE: {rmse}, R²: {r2}\")\n",
    "\n",
    "        # Feature Importance Analysis\n",
    "        feature_importances = best_clf.named_steps['classifier'].feature_importances_\n",
    "        column_transformer = best_clf.named_steps['preprocessor']\n",
    "        feature_names = get_transformer_feature_names(column_transformer)\n",
    "       \n",
    "        # Ensure the number of feature names matches the number of feature importances\n",
    "        if len(feature_names) == len(feature_importances):\n",
    "            feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "            sorted_feature_importance = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "            # Extracting and displaying top features excluding 'Main_language_missing'\n",
    "            top_languages1 = sorted_feature_importance[~sorted_feature_importance['Feature'].str.contains('Main_language_missing')]\n",
    "            top_languages = top_languages1[top_languages1['Feature'].str.contains('Main_language_')].head(6)\n",
    "            countries= sorted_feature_importance[sorted_feature_importance['Feature'].str.contains('Main_country_')]\n",
    "            top_countries = countries[~countries['Feature'].str.contains('missing')].head(6)\n",
    "\n",
    "            print(f\"Top 4 Languages for {genre}:\\n{top_languages}\")\n",
    "            print(f\"Top 4 Countries for {genre}:\\n{top_countries}\")\n",
    "        else:\n",
    "            print(\"Number of feature names and feature importances do not match.\")\n",
    "\n",
    "        import plotly.express as px\n",
    "        import plotly.offline as pyo\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        import mpld3\n",
    "\n",
    "        # List of countries to highlight\n",
    "        def clean_country_name(country_name):\n",
    "            cleaned_name = country_name.replace(\"Main_country_\", \"\").replace(\" of America\", \"\")\n",
    "            return cleaned_name\n",
    "\n",
    "        # Apply the function to each element in the list\n",
    "        cleaned_countries = [clean_country_name(country) for country in top_countries['Feature']]\n",
    "        # Generate a unique color for each country\n",
    "        colors = px.colors.qualitative.Plotly  # This is a list of colors provided by Plotly\n",
    "\n",
    "        # Ensure there are enough colors for all countries\n",
    "        assert len(colors) >= len(cleaned_countries), \"Not enough colors available.\"\n",
    "\n",
    "        # Create a DataFrame\n",
    "        importance_values = sorted_feature_importance  # Replace with your actual importance values\n",
    "       \n",
    "        df = pd.DataFrame({\n",
    "            'color': cleaned_countries,\n",
    "            'country': cleaned_countries  # Use country names as color identifiers for now\n",
    "        })\n",
    "\n",
    "        # Create a color mapping: each country gets a different color\n",
    "        color_mapping = {country: colors[i] for i, country in enumerate(cleaned_countries)}\n",
    "\n",
    "        # Create the interactive world map\n",
    "        fig = px.choropleth(df,\n",
    "                            locations='color',\n",
    "                            locationmode='country names',\n",
    "                            color='country',\n",
    "                            hover_name='color',\n",
    "                            color_discrete_map=color_mapping,  # Use the color mapping\n",
    "                            title='Top 6 recommended release country')\n",
    "\n",
    "        # Show the figure\n",
    "        fig.show()\n",
    "        # Save the figure to an HTML file\n",
    "        if genre == 'Action/Adventure':\n",
    "            pyo.plot(fig, filename=f'catboost_info/mapactionadventure.html', auto_open=False)\n",
    "        else:\n",
    "            pyo.plot(fig, filename=f'catboost_info/map_{genre}.html', auto_open=False)\n",
    "\n",
    "        # Create a beautiful bar plot    \n",
    "        # Create a bar plot using Plotly Express\n",
    "        \n",
    "        top_languages['Feature'] = top_languages['Feature'].str.replace('Main_language_', '')\n",
    "        top_languages['Feature'] = top_languages['Feature'].str.replace('Language', '')\n",
    "\n",
    "        fig2 = px.bar(top_languages.head(5),\n",
    "                    x='Importance', \n",
    "                    y='Feature',\n",
    "                    title='Top 6 Important Language',\n",
    "                    labels={'Importance': 'Language Importance', 'Feature': 'Languages'},\n",
    "                    orientation='h',  # Horizontal bar plot\n",
    "                    color='Importance',  # Color the bars by the 'Importance' value\n",
    "                    color_continuous_scale='viridis')  # Use the 'viridis' color scale\n",
    "\n",
    "        # Update layout for better readability\n",
    "        fig2.update_layout(\n",
    "            xaxis_title='Language Importance',\n",
    "            yaxis_title='Languages',\n",
    "            title_font_size=15,\n",
    "            xaxis_tickangle=45\n",
    "        )\n",
    "\n",
    "        # Show the plot\n",
    "        fig2.show()\n",
    "\n",
    "        if genre == 'Action/Adventure':\n",
    "            pyo.plot(fig2, filename=f'catboost_info/maplangueactionadventure.html', auto_open=False)\n",
    "\n",
    "        else:\n",
    "            pyo.plot(fig2, filename=f'catboost_info/maplangue_{genre}.html', auto_open=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while analyzing genre {genre}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cast : find best profile for each of the top 4 roles (sex, number of movies played in, age)\n",
    "In the code below we trained XGB(boosted trees) model such that it can predicts success rate based on thses features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Load dataset from a specified filepath.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_data(df,genre):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset: drop missing values, filter by 'Successful' column,\n",
    "    calculate actors' ages, and encode categorical data. Only keep a specified Genre.\n",
    "    \"\"\"\n",
    "    df.dropna(inplace=True)\n",
    "    df = df[df['Top_genres'] == genre]\n",
    "\n",
    "    #df = df[df['Successful'] >= 7.5]\n",
    "\n",
    "    df['Release_Date'] = pd.to_datetime(df['Release_Date'], format='%Y')\n",
    "\n",
    "\n",
    "    # Ensure that birth year columns are numeric\n",
    "    for role in ['role_1', 'role_2', 'role_3', 'role_4']:\n",
    "        birth_year_column = f'{role}_birth_year'\n",
    "        df[birth_year_column] = pd.to_numeric(df[birth_year_column], errors='coerce')\n",
    "        #we use pd.to_numeric with errors='coerce' to convert the birth year columns to numeric types\n",
    "\n",
    "    # Calculating ages of actors from their birth year and movie release year\n",
    "    df['role_1_age'] = df['Release_Date'].dt.year - df['role_1_birth_year']\n",
    "    df['role_2_age'] = df['Release_Date'].dt.year - df['role_2_birth_year']\n",
    "    df['role_3_age'] = df['Release_Date'].dt.year - df['role_3_birth_year']\n",
    "    df['role_4_age'] = df['Release_Date'].dt.year - df['role_4_birth_year']\n",
    "\n",
    "    df['role_1_roles_count'] =  (df['role_1_roles_count'] * df['role_1_age']-30) / 60 \n",
    "    df['role_2_roles_count'] =  (df['role_1_roles_count'] * df['role_2_age']-30) / 60\n",
    "    df['role_3_roles_count'] =  (df['role_1_roles_count'] * df['role_3_age']-30) / 60\n",
    "    df['role_4_roles_count'] =  (df['role_1_roles_count'] * df['role_4_age']-30) / 60\n",
    "\n",
    "    # Encoding categorical data\n",
    "    label_encoder = LabelEncoder()\n",
    "    for role in ['role_1_sex', 'role_2_sex', 'role_3_sex', 'role_4_sex']:\n",
    "        df[role] = label_encoder.fit_transform(df[role].astype(str))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_selection(df):\n",
    "    \"\"\"\n",
    "    Select and return features and target variable from the dataframe.\n",
    "    \"\"\"\n",
    "    features = df[['role_1_age', 'role_2_age', 'role_3_age', 'role_4_age',\n",
    "                   'role_1_roles_count', 'role_2_roles_count', \n",
    "                   'role_3_roles_count', 'role_4_roles_count',\n",
    "                   'role_1_sex', 'role_2_sex', 'role_3_sex', 'role_4_sex']]\n",
    "    target = df['Successful']\n",
    "\n",
    "    # Align features and target by index to ensure they match\n",
    "    features.dropna(inplace=True)\n",
    "    target = target.loc[features.index]\n",
    "\n",
    "    return features, target\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train the model using GridSearchCV with XGBoost.\n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_child_weight': [1, 2, 3],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'reg_alpha': [0, 0.1, 0.2]\n",
    "    }\n",
    "    param_grid2 = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_child_weight': [1, 2, 3],\n",
    "        'subsample': [0.7, 0.8, 0.9]\n",
    "    }\n",
    "\n",
    "    xgb = XGBRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid2, \n",
    "                           cv=3, n_jobs=-1, verbose=0, \n",
    "                           scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set and return the MSE and R² score.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    return mse, r2\n",
    "\n",
    "def feature_importance(model, features):\n",
    "    \"\"\"\n",
    "    Generate a DataFrame of feature importances.\n",
    "    \"\"\"\n",
    "    feature_importances = model.feature_importances_\n",
    "    importance_df = pd.DataFrame({'Feature': features.columns, 'Importance': feature_importances})\n",
    "    return importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "########################################################################################################################\n",
    "#Purpose of the random search algorithm below is to find best combination of feature that maximisze success rate based on the importance of the parameters find above\n",
    "\n",
    "def convert_profile_to_model_input(profile):\n",
    "    # Convert the profile dictionary to a list or array in the order expected by the model\n",
    "    # This function needs to be defined based on how your model expects the input\n",
    "    # For example:\n",
    "    model_input = []\n",
    "    for role, features in profile.items():\n",
    "        model_input.append(features['age'])\n",
    "        model_input.append(features['roles_count'])\n",
    "        model_input.append(1 if features['sex'] == 'M' else 0)  # Assuming sex is binary and encoded as 1/0\n",
    "    return model_input\n",
    "\n",
    "def optimize_profiles(model, initial_guess, iterations=1000):\n",
    "    \"\"\"\n",
    "    Optimizes actor profiles to achieve the best success rate predicted by the model.\n",
    "    \n",
    "    Args:\n",
    "    model: Trained machine learning model used for prediction.\n",
    "    initial_guess: Initial guess of the profiles.\n",
    "    iterations: Number of iterations for the optimization process.\n",
    "    \n",
    "    Returns:\n",
    "    Tuple of the best profile and its corresponding success rate.\n",
    "    \"\"\"\n",
    "    best_profile = initial_guess.copy()\n",
    "    best_score = -np.inf\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        current_profile = best_profile.copy()\n",
    "\n",
    "        # Randomly adjust one parameter in one of the roles\n",
    "        role_to_adjust = random.choice(list(current_profile.keys()))\n",
    "        feature_to_adjust = random.choice(list(current_profile[role_to_adjust].keys()))\n",
    "\n",
    "        if feature_to_adjust == 'age':\n",
    "            current_profile[role_to_adjust]['age'] = random.choice(range(15,80))\n",
    "        elif feature_to_adjust == 'roles_count':\n",
    "            current_profile[role_to_adjust]['roles_count'] = random.choice(range(1,300))\n",
    "        elif feature_to_adjust == 'sex':\n",
    "            current_profile[role_to_adjust]['sex'] = random.choice(sex_range)\n",
    "\n",
    "        # Convert the current profile for model input and predict success rate\n",
    "        model_input = convert_profile_to_model_input(current_profile)\n",
    "        success_rate = model.predict([model_input])[0]\n",
    "\n",
    "        # Update best profile if current is better\n",
    "        if success_rate > best_score:\n",
    "            best_score = success_rate\n",
    "            best_profile = current_profile\n",
    "\n",
    "    return best_profile, best_score\n",
    "\n",
    "########################################################################################################################\n",
    "def similarity_score(actor_data, target_profile):\n",
    "    age_diff = abs(actor_data['age'] - target_profile['age'])\n",
    "    roles_count_diff = abs(actor_data['total_roles'] - target_profile['roles_count'])\n",
    "    sex_match = actor_data['category'] == target_profile['sex']\n",
    "    return age_diff * 0.5 + roles_count_diff * 0.5 + (0 if sex_match else 1000)\n",
    "\n",
    "\n",
    "def find_best_matches(df, optimized_profile):\n",
    "    \"\"\"\n",
    "    Finds the top 2 best matching actors for the given optimized profile.\n",
    "    \n",
    "    Args:\n",
    "    df: DataFrame containing actor information.\n",
    "    optimized_profile: Dictionary of the optimized profiles.\n",
    "    \n",
    "    Returns:\n",
    "    Dictionary of top 2 best matching actors for each role.\n",
    "    \"\"\"\n",
    "    top2_matches = {}\n",
    "    for role, profile in optimized_profile.items():\n",
    "        # Calculate similarity score for each actor in the dataframe\n",
    "        df['similarity_score'] = df.apply(lambda row: similarity_score(row, profile), axis=1)\n",
    "        \n",
    "        # Sort the dataframe by similarity score and select the top 2 actors\n",
    "        top2 = df.sort_values(by='similarity_score').head(2)\n",
    "        \n",
    "        # Store the names of the top 2 actors for each role\n",
    "        top2_matches[role] = top2['name'].tolist()\n",
    "\n",
    "    return top2_matches\n",
    "\n",
    "\n",
    "def get_actor_names(nconst_lists, cast_df):\n",
    "    \"\"\"\n",
    "    Retrieves actor names corresponding to the nconst values for each role. \n",
    "    If the primary choice (first nconst) leads to an IndexError, it tries the secondary choice, and so on.\n",
    "    \n",
    "    Args:\n",
    "    nconst_lists: Dictionary of lists of nconst values for actors for each role.\n",
    "    cast_df: DataFrame containing cast names and nconst.\n",
    "    \n",
    "    Returns:\n",
    "    Dictionary of actor names for each role.\n",
    "    \"\"\"\n",
    "    primary_names = {role: cast_name_df.loc[cast_name_df['nconst'] == actor_id, 'primaryName'].values[0] \n",
    "                 for role, actor_id in similar_actors.items() if cast_name_df['nconst'].str.contains(actor_id).any()}\n",
    "    \n",
    "    return primary_names\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = '../Datasets/cast_processed_corrected_genres.csv'\n",
    "    df = load_data(filepath)\n",
    "    Result = {}\n",
    "\n",
    "    if df is not None:\n",
    "        Top_Genre =['Drama','Other','Family', 'Action/Adventure', 'Horror', 'Fiction']\n",
    "        for genre in Top_Genre:\n",
    "            print(f\"Analyzing genre: {genre} - Data Points: {len(df[df['Top_genres'] == genre])}\")\n",
    "\n",
    "            ######ﬁ#############################################################################################################\n",
    "            ######ﬁ###################### XGB to understand importance of each features ########################################\n",
    "            ######ﬁ#############################################################################################################\n",
    "            \n",
    "            df_processed = preprocess_data(df,genre)\n",
    "            features, target = feature_selection(df_processed)\n",
    "\n",
    "            # Splitting the dataset\n",
    "            X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "            X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "            X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "            # You can fill NaN values with the mean (or any other appropriate value)\n",
    "            X_train.fillna(X_train.mean(), inplace=True)\n",
    "            X_test.fillna(X_test.mean(), inplace=True)\n",
    "\n",
    "            # Feature scaling\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "            # Model training\n",
    "            best_model = train_model(X_train_scaled, y_train)\n",
    "\n",
    "            # Model evaluation\n",
    "            mse_best, r2_best = evaluate_model(best_model, X_test_scaled, y_test)\n",
    "            print(f\"Optimized XGBoost MSE: {mse_best}, R²: {r2_best}\")\n",
    "\n",
    "            # Feature importance\n",
    "            importance_df = feature_importance(best_model, features)\n",
    "\n",
    "            # Print out the result\n",
    "            roles = ['role_1', 'role_2', 'role_3', 'role_4']\n",
    "            profiles = {}\n",
    "\n",
    "            for role in roles:\n",
    "                role_features = importance_df[importance_df['Feature'].str.startswith(role)]\n",
    "                profiles[role] = role_features.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "            for role, profile in profiles.items():\n",
    "                print(f\"{role} Profile:\")\n",
    "                print(profile)\n",
    "                print()\n",
    "\n",
    "            ######ﬁ#############################################################################################################\n",
    "            ######ﬁ################ find best combination of feature that maximisze success rate ###############################\n",
    "            ######ﬁ#############################################################################################################\n",
    "\n",
    "            #Purpose of the random search algorithm below is to find best combination of feature that maximisze success\n",
    "            #rate based on the importance of the parameters found\n",
    "\n",
    "            # Define the range of values and initial guesses\n",
    "            age_range = range(10, 90)  # Age from 10 to 90\n",
    "            role_count_range = range(1, 150)  # Role count from 1 to 150\n",
    "            sex_range = ['M', 'F']  # Male and Female\n",
    "\n",
    "            initial_guess = {\n",
    "                'role_1': {'age': 0, 'roles_count': 0, 'sex': 'M'},\n",
    "                'role_2': {'age': 0, 'roles_count': 0, 'sex': 'M'},\n",
    "                'role_3': {'age': 0, 'roles_count': 0, 'sex': 'M'},\n",
    "                'role_4': {'age': 0, 'roles_count': 0, 'sex': 'M'}\n",
    "            }\n",
    "            \n",
    "            # Finding similar actors for each role\n",
    "            optimized_profile, optimized_score = optimize_profiles(best_model, initial_guess, iterations=50000)\n",
    "            print(\"Optimized Profile:\", optimized_profile)\n",
    "            \n",
    "            ####################################################################################################################\n",
    "            ##################### Looking for the best match in the database in order to find Actors ###########################\n",
    "            ####################################################################################################################\n",
    "\n",
    "            try:\n",
    "                cast_name_df = pd.read_csv('../Datasets/top1000_actors.csv')\n",
    "                similar_actors = find_best_matches(cast_name_df, optimized_profile)\n",
    "                print(\"Best Actors:\", similar_actors)\n",
    "\n",
    "            except FileNotFoundError as e:\n",
    "                print(f\"File not found: {e}\")\n",
    "\n",
    "\n",
    "            # Plotting Feature Importance\n",
    "            top_features = importance_df.head(16)\n",
    "\n",
    "            # Creating the bar plot with Plotly Express\n",
    "            fig3 = px.bar(top_features,\n",
    "              x='Importance', \n",
    "              y='Feature',\n",
    "              title=f'Important Features for Movie Success in {genre} : Actors',\n",
    "              labels={'Importance': 'Feature Importance', 'Feature': 'Features'},\n",
    "              orientation='h',  # Horizontal bar plot\n",
    "              color='Importance',  # Color the bars by the 'Importance' value\n",
    "              color_continuous_scale='viridis')  # Color scale for the bars\n",
    "\n",
    "            # Displaying the plot\n",
    "            fig3.show()\n",
    "\n",
    "            if genre == 'Action/Adventure':\n",
    "                pyo.plot(fig3, filename=f'catboost_info/featureimportance_actionadventure.html', auto_open=False)\n",
    "            else:\n",
    "                pyo.plot(fig3, filename=f'catboost_info/featureimportance_{genre}.html', auto_open=False)\n",
    "\n",
    "        print(Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Director : Find best profil based on the number of movies and the Director age "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def read_data(file_path):\n",
    "    try:\n",
    "        return pd.read_csv(file_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e}\")\n",
    "        return None\n",
    "\n",
    "def preprocess_data(df, genre):\n",
    "    df_genre = df[df['Top_genres'] == genre]\n",
    "    df_genre = df_genre[['num_movies', 'Director_age', 'Successful', 'death_year', 'Director_name','Top_genres']].dropna()\n",
    "    df_genre = df_genre[df_genre['Director_age'] >= 20]  # realistic age range\n",
    "    return df_genre\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    model = XGBRegressor(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    #print(f\"MSE: {mse}, R²: {r2}\")\n",
    "\n",
    "def find_optimal_combination(model):\n",
    "    best_score = -np.inf\n",
    "    best_combination = (0, 0)\n",
    "\n",
    "    for movies in range(1, 101):\n",
    "        for age in range(20, 81):\n",
    "            sample = np.array([[movies, age]])\n",
    "            predicted_success = model.predict(sample)[0]\n",
    "            if predicted_success > best_score:\n",
    "                best_score = predicted_success\n",
    "                best_combination = (movies, age)\n",
    "\n",
    "    return best_combination\n",
    "# Load the data\n",
    "movies_df = read_data('../Datasets/movies_directors_with_genre.csv')\n",
    "if movies_df is None:\n",
    "    raise Exception(\"Data file not found. Please check the file path.\")\n",
    "\n",
    "# Define genres to analyze\n",
    "genres = ['Drama', 'Family', 'Action/Adventure', 'Horror', 'Fiction']\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "optimal_directors_by_genre = {}\n",
    "feature_importances_by_genre = {}\n",
    "optimal_combinations = {}\n",
    "processed_data_by_genre = {}\n",
    "\n",
    "for genre in genres:\n",
    "    df_genre = preprocess_data(movies_df, genre)\n",
    "    processed_data_by_genre[genre] = df_genre  # Store the processed data for visualization\n",
    "    X = df_genre[['num_movies', 'Director_age']]\n",
    "    y = df_genre['Successful']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    best_model = train_model(X_train, y_train)\n",
    "    evaluate_model(best_model, X_test, y_test)\n",
    "    \n",
    "    best_combination = find_optimal_combination(best_model)\n",
    "    optimal_combinations[genre] = best_combination\n",
    "\n",
    "    # Store feature importances\n",
    "    feature_importances = best_model.feature_importances_\n",
    "    importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})\n",
    "    feature_importances_by_genre[genre] = importance_df\n",
    "\n",
    "    # Filter directors who are alive and whose profile is closest to the optimal combination\n",
    "\n",
    "    df_genre = df_genre[df_genre['death_year'] == 0]\n",
    "\n",
    "    df_genre['similarity'] = df_genre.apply(lambda row: 1/ np.sqrt((row['num_movies'] - best_combination[0])**2 + (row['Director_age'] - best_combination[1])**2), axis=1)\n",
    "    best_director = []\n",
    "\n",
    "    best_director.append(df_genre.sort_values(by='similarity', ascending=False).iloc[0]['Director_name'])\n",
    "    if genre == 'Drama' or genre == 'Family':\n",
    "        #For those genre iloc[0] == iloc[1] there are doublons, so to avoid that we just do that \n",
    "        best_director.append(df_genre.sort_values(by='similarity', ascending=False).iloc[3]['Director_name'])\n",
    "    else :\n",
    "        best_director.append(df_genre.sort_values(by='similarity', ascending=False).iloc[1]['Director_name'])\n",
    "\n",
    "\n",
    "    optimal_directors_by_genre[genre] = best_director\n",
    "\n",
    "print(\"Best Directors for Each Genre:\", optimal_directors_by_genre)\n",
    "\n",
    "# Visualization\n",
    "for genre, df_genre in processed_data_by_genre.items():\n",
    "\n",
    "    fig = px.scatter(df_genre, x='num_movies', y='Successful', title=f'Relationship Between Number of Movies and Success in {genre}')\n",
    "    fig.show()\n",
    "\n",
    "    fig2 = px.scatter(df_genre, x='Director_age', y='Successful', title=f'Relationship Between Director Age and Success in {genre}')\n",
    "    fig2.show()\n",
    "    if genre == 'Action/Adventure':\n",
    "        pyo.plot(fig, filename=f'catboost_info/relationnummovie_actionadventure.html', auto_open=False)\n",
    "        pyo.plot(fig2, filename=f'catboost_info/relationdirectorage_actionadventure.html', auto_open=False)\n",
    "\n",
    "    else:\n",
    "        pyo.plot(fig, filename=f'catboost_info/relationnummovie_{genre}.html', auto_open=False)\n",
    "        pyo.plot(fig2, filename=f'catboost_info/relationdirectorage_{genre}.html', auto_open=False)\n",
    "\n",
    "\n",
    "\n",
    "# Feature Importance Plot\n",
    "for genre, importance_df in feature_importances_by_genre.items():\n",
    "    fig = px.bar(importance_df,\n",
    "                 x='Importance', \n",
    "                 y='Feature',\n",
    "                 title=f'Important Features for Movie Success in {genre}',\n",
    "                 labels={'Importance': 'Feature Importance', 'Feature': 'Features'},\n",
    "                 orientation='h', \n",
    "                 color='Importance', \n",
    "                 color_continuous_scale='viridis')\n",
    "    fig.show()\n",
    "    if genre == 'Action/Adventure':\n",
    "        pyo.plot(fig, filename=f'catboost_info/actorimportance_actionadventure.html', auto_open=False)\n",
    "\n",
    "    else:\n",
    "        pyo.plot(fig, filename=f'catboost_info/actorimportance_{genre}.html', auto_open=False)\n",
    "\n",
    "# Optimal Combinations Plot\n",
    "optimal_data = []\n",
    "for genre, comb in optimal_combinations.items():\n",
    "    optimal_data.append({'Genre': genre, 'num_movies': comb[0], 'Director_age': comb[1]})\n",
    "\n",
    "optimal_df = pd.DataFrame(optimal_data)\n",
    "fig2 = px.scatter(optimal_df, x='num_movies', y='Director_age', color='Genre',\n",
    "                  title='Optimal Number of Movies and Director Age for Each Genre',\n",
    "                  labels={'num_movies': 'Number of Movies', 'Director_age': 'Director Age'})\n",
    "fig2.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
